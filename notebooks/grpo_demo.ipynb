{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c472c9d2",
   "metadata": {},
   "source": [
    "# üß† GRPO CPU Demo - Quick Start Guide\n",
    "\n",
    "This notebook demonstrates how to use the GRPO-based reinforcement fine-tuning system on CPU hardware.\n",
    "\n",
    "## What is GRPO?\n",
    "\n",
    "Group Relative Policy Optimization (GRPO) is an advanced reinforcement learning algorithm that:\n",
    "- Works efficiently on CPU hardware\n",
    "- Doesn't require a separate value function (unlike PPO)\n",
    "- Uses relative rewards for better stability\n",
    "- Shows improvements quickly (often within 20-30 steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8eb9ab",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "First, let's check our environment and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79244d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python and PyTorch installation\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Please run: pip install torch --index-url https://download.pytorch.org/whl/cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6267570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check key packages\n",
    "packages = [\"transformers\", \"datasets\", \"trl\", \"gradio\", \"accelerate\"]\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8018056",
   "metadata": {},
   "source": [
    "## üöÄ Quick Demo: Training a Model with GRPO\n",
    "\n",
    "Let's demonstrate the core functionality by training a small model on math problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our GRPO trainer\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from training.grpo_trainer import CPUGRPOTrainer, CPUGRPOConfig\n",
    "from utils.grpo_utils import RewardFunctions, DatasetProcessor\n",
    "\n",
    "print(\"‚úÖ GRPO components imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ed10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the trainer for a quick demo\n",
    "config = CPUGRPOConfig(\n",
    "    model_name=\"distilgpt2\",  # Small model for quick demo\n",
    "    max_length=128,\n",
    "    batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"./demo_output\"\n",
    ")\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Max length: {config.max_length}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Output directory: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ccd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "print(\"üîß Initializing GRPO trainer...\")\n",
    "trainer = CPUGRPOTrainer(config)\n",
    "print(\"‚úÖ Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model before training\n",
    "test_prompt = \"Solve this math problem: What is 2 + 2?\"\n",
    "\n",
    "print(\"üß™ Testing base model:\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "\n",
    "base_response = trainer.generate_response(test_prompt, max_new_tokens=50)\n",
    "print(f\"Base model response: {base_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da406370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small dataset for quick training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Simple math problems for demonstration\n",
    "math_prompts = [\n",
    "    \"Solve this: 1 + 1 = ?\",\n",
    "    \"What is 3 + 2?\",\n",
    "    \"Calculate: 5 - 3 = ?\",\n",
    "    \"What is 2 √ó 4?\",\n",
    "    \"Solve: 10 √∑ 2 = ?\",\n",
    "] * 10  # Repeat for more training data\n",
    "\n",
    "dataset = Dataset.from_dict({\"prompt\": math_prompts})\n",
    "print(f\"üìö Created dataset with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reward function for math problems\n",
    "reward_fn = RewardFunctions.math_reasoning_reward\n",
    "\n",
    "# Test the reward function\n",
    "test_responses = [\n",
    "    \"The answer is 4\",\n",
    "    \"I don't know\",\n",
    "    \"Let me calculate: 2 + 2 = 4\"\n",
    "]\n",
    "\n",
    "rewards = reward_fn(test_responses)\n",
    "print(\"üéØ Reward function test:\")\n",
    "for response, reward in zip(test_responses, rewards):\n",
    "    print(f\"  '{response}' ‚Üí Reward: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3edc1b",
   "metadata": {},
   "source": [
    "## üéì Training the Model\n",
    "\n",
    "**Note**: This is a simplified demo. For a full training run, use the web interface or command-line script.\n",
    "\n",
    "Due to the complexity of setting up the full TRL training in a notebook, we'll demonstrate the components. For actual training, please use:\n",
    "\n",
    "```bash\n",
    "python app.py  # Web interface\n",
    "# or\n",
    "python train_grpo.py --samples 100 --dataset gsm8k\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the training setup (without actually training)\n",
    "print(\"üéì Training setup demonstration:\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Task: Mathematical reasoning\")\n",
    "print(f\"  Reward function: Math-specific rewards\")\n",
    "print(\"\\n‚ö†Ô∏è  For actual training, please use the web interface or command-line script\")\n",
    "print(\"   This ensures proper memory management and progress tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ac8e4",
   "metadata": {},
   "source": [
    "## üåê Web Interface Demo\n",
    "\n",
    "The easiest way to use this system is through the web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to launch the web interface\n",
    "print(\"üåê To launch the web interface:\")\n",
    "print(\"\")\n",
    "print(\"1. Open a terminal/command prompt\")\n",
    "print(\"2. Navigate to the project directory\")\n",
    "print(\"3. Run: python app.py\")\n",
    "print(\"4. Open http://localhost:7860 in your browser\")\n",
    "print(\"\")\n",
    "print(\"The web interface provides:\")\n",
    "print(\"  ‚Ä¢ üéØ Training Setup - Configure and start training\")\n",
    "print(\"  ‚Ä¢ üß™ Model Testing - Test and compare models\")\n",
    "print(\"  ‚Ä¢ üìö Examples & Help - Documentation and examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4a29e",
   "metadata": {},
   "source": [
    "## üîß Command-Line Usage\n",
    "\n",
    "For programmatic usage, you can use the command-line script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08166463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show command-line options\n",
    "print(\"üíª Command-line training examples:\")\n",
    "print(\"\")\n",
    "print(\"# Basic training on GSM8K dataset:\")\n",
    "print(\"python train_grpo.py --dataset gsm8k --samples 200 --lr 1e-5\")\n",
    "print(\"\")\n",
    "print(\"# Custom training with specific parameters:\")\n",
    "print(\"python train_grpo.py --model distilgpt2 --task general --epochs 2\")\n",
    "print(\"\")\n",
    "print(\"# Get help with all options:\")\n",
    "print(\"python train_grpo.py --help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c08fab",
   "metadata": {},
   "source": [
    "## üìä Understanding GRPO Results\n",
    "\n",
    "When you run GRPO training, you'll see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe12ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of what training logs look like\n",
    "example_logs = [\n",
    "    {\"step\": 1, \"loss\": 2.45, \"mean_reward\": -0.2},\n",
    "    {\"step\": 10, \"loss\": 1.89, \"mean_reward\": 0.1},\n",
    "    {\"step\": 20, \"loss\": 1.23, \"mean_reward\": 0.4},\n",
    "    {\"step\": 30, \"loss\": 0.87, \"mean_reward\": 0.7},\n",
    "]\n",
    "\n",
    "print(\"üìà Example training progress:\")\n",
    "print(\"Step | Loss  | Mean Reward | Notes\")\n",
    "print(\"-----|-------|-------------|------\")\n",
    "for log in example_logs:\n",
    "    step = log[\"step\"]\n",
    "    loss = log[\"loss\"]\n",
    "    reward = log[\"mean_reward\"]\n",
    "    \n",
    "    if step == 1:\n",
    "        note = \"Starting training\"\n",
    "    elif step == 20:\n",
    "        note = \"'Aha moment' - major improvement\"\n",
    "    elif step == 30:\n",
    "        note = \"Good convergence\"\n",
    "    else:\n",
    "        note = \"Gradual improvement\"\n",
    "    \n",
    "    print(f\"{step:4d} | {loss:.2f}  | {reward:+.1f}         | {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb286b7",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "1. **GRPO is CPU-friendly**: No GPU required for training\n",
    "2. **Quick results**: Often shows improvement within 20-30 steps\n",
    "3. **Math tasks work well**: Specialized reward functions for mathematical reasoning\n",
    "4. **Easy to use**: Web interface for non-technical users\n",
    "5. **Flexible**: Command-line interface for advanced users\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Try the web interface**: `python app.py`\n",
    "2. **Experiment with different models**: Try `Qwen/Qwen2-0.5B-Instruct` or `distilgpt2`\n",
    "3. **Create custom reward functions**: Modify `src/utils/grpo_utils.py`\n",
    "4. **Scale up**: Use larger datasets and longer training\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "- [GRPO Paper](https://docs.unsloth.ai/basics/reinforcement-learning-guide)\n",
    "- [Gradio Documentation](https://gradio.app/docs)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy training! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
