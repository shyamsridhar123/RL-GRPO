{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98fb6c7",
   "metadata": {},
   "source": [
    "# LLM Reinforcement Learning - Getting Started\n",
    "\n",
    "This notebook demonstrates how to set up and get started with Large Language Model (LLM) reinforcement learning. We'll cover:\n",
    "\n",
    "1. **Environment Setup**: Installing PyTorch CPU version and dependencies\n",
    "2. **Basic Components**: Understanding RL agents, environments, and models\n",
    "3. **Simple Example**: Creating a basic text generation environment\n",
    "4. **Training Demo**: Simple PPO training example\n",
    "\n",
    "Since you don't have CUDA, we'll use CPU-only PyTorch for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605ddc5",
   "metadata": {},
   "source": [
    "## 1. Check Current PyTorch Installation\n",
    "\n",
    "Let's first check if PyTorch is installed and whether CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c82d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world \n"
     ]
    }
   ],
   "source": [
    "print(\"hello world \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371d0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n",
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"Running on CPU\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch is not installed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6b78a",
   "metadata": {},
   "source": [
    "## 2. Install CPU-Only PyTorch with Conda\n",
    "\n",
    "Since CUDA is not available, we'll install the CPU-only version of PyTorch. Run the following command in your terminal or uncomment and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5fcef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the above commands in your terminal to install PyTorch and dependenciesRetrieving notices: done\n",
      "Channels:\n",
      " - pytorch\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could Not Find C:\\WINDOWS\\TEMP\\tmpu57mo79p.bat\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell to install PyTorch CPU version\n",
    "!conda install pytorch torchvision torchaudio cpuonly -c pytorch -y\n",
    "\n",
    "# Alternative: Install additional ML packages\n",
    "# !pip install transformers datasets accelerate gymnasium tensorboard wandb matplotlib seaborn\n",
    "\n",
    "print(\"Run the above commands in your terminal to install PyTorch and dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702dda6",
   "metadata": {},
   "source": [
    "## 3. Verify PyTorch Installation\n",
    "\n",
    "After installation, let's verify that PyTorch is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd32fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch Installation Verification ===\n",
      "PyTorch version: 2.5.1\n",
      "Python version: 2.5.1\n",
      "CUDA available: False\n",
      "Device: cpu\n",
      "\n",
      "Basic tensor operations working: True\n",
      "‚úÖ PyTorch is installed and working correctly!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== PyTorch Installation Verification ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {torch.version.__version__ if hasattr(torch.version, '__version__') else 'N/A'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "# Test basic operations\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "z = torch.matmul(x, y)\n",
    "\n",
    "print(f\"\\nBasic tensor operations working: {z.shape == (3, 3)}\")\n",
    "print(\"‚úÖ PyTorch is installed and working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb8938",
   "metadata": {},
   "source": [
    "## 4. Test PyTorch with Simple Tensor Operations\n",
    "\n",
    "Let's perform some basic tensor operations to ensure everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5cc6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tensor Operations Test ===\n",
      "Tensor a: tensor([1, 2, 3, 4])\n",
      "Tensor b shape: torch.Size([2, 3])\n",
      "Tensor c (zeros): \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Tensor arithmetic (a + 10): tensor([11, 12, 13, 14])\n",
      "\n",
      "Matrix multiplication result shape: torch.Size([3, 2])\n",
      "\n",
      "Neural network layer output shape: torch.Size([1, 2])\n",
      "\n",
      "‚úÖ All tensor operations successful!\n"
     ]
    }
   ],
   "source": [
    "# Create some tensors\n",
    "print(\"=== Tensor Operations Test ===\")\n",
    "\n",
    "# Basic tensor creation\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.randn(2, 3)\n",
    "c = torch.zeros(3, 3)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b shape: {b.shape}\")\n",
    "print(f\"Tensor c (zeros): \\n{c}\")\n",
    "\n",
    "# Basic operations\n",
    "result = a + 10\n",
    "print(f\"\\nTensor arithmetic (a + 10): {result}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "x = torch.randn(3, 4)\n",
    "y = torch.randn(4, 2)\n",
    "z = torch.mm(x, y)\n",
    "print(f\"\\nMatrix multiplication result shape: {z.shape}\")\n",
    "\n",
    "# Neural network layer test\n",
    "linear_layer = nn.Linear(4, 2)\n",
    "input_tensor = torch.randn(1, 4)\n",
    "output = linear_layer(input_tensor)\n",
    "print(f\"\\nNeural network layer output shape: {output.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ All tensor operations successful!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a8c27",
   "metadata": {},
   "source": [
    "## 5. Introduction to LLM Reinforcement Learning\n",
    "\n",
    "Now that PyTorch is working, let's explore the basics of LLM reinforcement learning:\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Environment**: Defines the text generation task and reward function\n",
    "2. **Agent**: The RL algorithm (e.g., PPO) that learns to generate better text\n",
    "3. **Model**: The language model (e.g., GPT-2) that generates text\n",
    "4. **Reward Function**: Evaluates the quality of generated text\n",
    "\n",
    "### Simple Example - Text Generation Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ac366",
   "metadata": {},
   "source": [
    "## 7. Next Steps and Resources\n",
    "\n",
    "Congratulations! You now have a working environment for LLM reinforcement learning. Here's what you can do next:\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Explore the Project Structure**: Check out the `src/` directory for:\n",
    "   - `agents/`: PPO and other RL algorithms\n",
    "   - `models/`: Language model wrappers for RL\n",
    "   - `environments/`: Text generation environments\n",
    "   - `training/`: Training and evaluation scripts\n",
    "\n",
    "2. **Run Training Examples**:\n",
    "   ```bash\n",
    "   python src/training/train_rl_agent.py --config configs/ppo_config.yaml\n",
    "   ```\n",
    "\n",
    "3. **Experiment with Different Models**:\n",
    "   - Try different base models (GPT-2, DistilGPT-2)\n",
    "   - Experiment with different reward functions\n",
    "   - Test various RL algorithms\n",
    "\n",
    "### üìö Key Concepts to Learn:\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)**: The main RL algorithm we use\n",
    "- **Policy Gradient Methods**: How RL agents learn from rewards\n",
    "- **Text Generation Environments**: Defining tasks and rewards\n",
    "- **Language Model Fine-tuning**: Adapting models for specific tasks\n",
    "\n",
    "### üîó Useful Resources:\n",
    "\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [OpenAI Spinning Up in Deep RL](https://spinningup.openai.com/)\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347)\n",
    "- [RLHF Paper](https://arxiv.org/abs/2203.02155)\n",
    "\n",
    "### üõ†Ô∏è Development Tips:\n",
    "\n",
    "- Start with simple environments and small models\n",
    "- Monitor training with TensorBoard or Weights & Biases\n",
    "- Use CPU for prototyping, GPU for larger experiments\n",
    "- Experiment with different reward functions\n",
    "\n",
    "Happy learning! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0646ff6",
   "metadata": {},
   "source": [
    "## 8. GRPO vs PPO: Better Choice for CPU Training\n",
    "\n",
    "You're absolutely right! **GRPO (Group Relative Policy Optimization)** is often a better choice than PPO, especially for CPU training of LLMs. Here's why:\n",
    "\n",
    "### üöÄ Why GRPO is Better for CPU:\n",
    "\n",
    "1. **Memory Efficiency**: GRPO uses relative rewards which reduces memory footprint\n",
    "2. **Computational Efficiency**: Less complex advantage estimation compared to PPO\n",
    "3. **Stable Training**: More stable convergence, especially with limited compute\n",
    "4. **Batch Processing**: Better suited for CPU batch processing patterns\n",
    "\n",
    "### üîÑ GRPO vs PPO Key Differences:\n",
    "\n",
    "| Aspect | PPO | GRPO |\n",
    "|--------|-----|------|\n",
    "| **Memory Usage** | Higher (stores values, advantages) | Lower (relative rewards) |\n",
    "| **Computation** | Complex GAE calculation | Simpler relative comparisons |\n",
    "| **Stability** | Can be unstable with large batches | More stable across batch sizes |\n",
    "| **CPU Performance** | Moderate | Better optimized |\n",
    "\n",
    "### üí° When to Use Each:\n",
    "\n",
    "- **Use GRPO**: CPU training, limited memory, need stability\n",
    "- **Use PPO**: GPU clusters, complex environments, established pipelines\n",
    "\n",
    "Let's implement a simple GRPO agent to compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Azure ML Compute - Hello World\n",
      "=============================================\n",
      "üìù Created test script: test_compute.py\n",
      "üí° Next steps:\n",
      "   1. Submit this script to your Azure ML compute cluster\n",
      "   2. Use Azure ML extension or CLI to run: az ml job create\n",
      "   3. Check output to verify compute is working\n",
      "\n",
      "üîó Test script contents:\n",
      "\n",
      "import sys\n",
      "import torch\n",
      "import platform\n",
      "\n",
      "print(\"=== Hello World from Azure ML Compute ===\")\n",
      "print(f\"Python version: {sys.version}\")\n",
      "print(f\"Platform: {platform.platform()}\")\n",
      "print(f\"PyTorch version: {torch.__version__}\")\n",
      "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
      "print(f\"CPU count: {torch.get_num_threads()}\")\n",
      "print(\"‚úÖ Compute is working correctly!\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Azure ML Compute with Hello World\n",
    "# Simple test to verify your compute cluster is working\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"üß™ Testing Azure ML Compute - Hello World\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create a simple test script\n",
    "test_script = \"\"\"\n",
    "import sys\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"=== Hello World from Azure ML Compute ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CPU count: {torch.get_num_threads()}\")\n",
    "print(\"‚úÖ Compute is working correctly!\")\n",
    "\"\"\"\n",
    "\n",
    "# Write test script to file\n",
    "test_file = \"test_compute.py\"\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "print(f\"üìù Created test script: {test_file}\")\n",
    "print(f\"üí° Next steps:\")\n",
    "print(f\"   1. Submit this script to your Azure ML compute cluster\")\n",
    "print(f\"   2. Use Azure ML extension or CLI to run: az ml job create\")\n",
    "print(f\"   3. Check output to verify compute is working\")\n",
    "\n",
    "print(f\"\\nüîó Test script contents:\")\n",
    "print(test_script)\n",
    "\n",
    "# Submit Hello World Job to Azure ML Compute Instance\n",
    "# This actually connects to your workspace and runs on your compute\n",
    "\n",
    "from azure.ai.ml import MLClient, command\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Submitting Hello World to Azure ML Compute Instance\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# You need to fill these in with your actual values\n",
    "SUBSCRIPTION_ID = \"your-subscription-id\"\n",
    "RESOURCE_GROUP = \"your-resource-group\" \n",
    "WORKSPACE_NAME = \"your-workspace-name\"\n",
    "COMPUTE_NAME = \"your-compute-instance-name\"  # NOT a cluster - single compute instance\n",
    "\n",
    "try:\n",
    "    # Connect to workspace\n",
    "    ml_client = MLClient(\n",
    "        DefaultAzureCredential(),\n",
    "        subscription_id=SUBSCRIPTION_ID,\n",
    "        resource_group_name=RESOURCE_GROUP,\n",
    "        workspace_name=WORKSPACE_NAME\n",
    "    )\n",
    "    \n",
    "    # Create hello world script\n",
    "    hello_script = \"\"\"\n",
    "import sys\n",
    "import torch\n",
    "import platform\n",
    "import os\n",
    "\n",
    "print(\"=== Hello World from Azure ML Compute Instance ===\")\n",
    "print(f\"Host: {platform.node()}\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CPU cores: {os.cpu_count()}\")\n",
    "print(\"‚úÖ Your compute instance is working!\")\n",
    "\"\"\"\n",
    "    \n",
    "    # Write script to file\n",
    "    os.makedirs(\"./hello_job\", exist_ok=True)\n",
    "    with open(\"./hello_job/hello.py\", \"w\") as f:\n",
    "        f.write(hello_script)\n",
    "    \n",
    "    # Create job\n",
    "    job = command(\n",
    "        code=\"./hello_job\",\n",
    "        command=\"python hello.py\",\n",
    "        compute=COMPUTE_NAME,\n",
    "        environment=\"azureml://registries/azureml/environments/sklearn-1.0/versions/1\"\n",
    "    )\n",
    "    \n",
    "    # Submit job\n",
    "    submitted_job = ml_client.jobs.create_or_update(job)\n",
    "    print(f\"‚úÖ Job submitted: {submitted_job.name}\")\n",
    "    print(f\"üîó View in studio: {submitted_job.studio_url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Please update the workspace details above with your actual values\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
